# -*- coding: utf-8 -*-
"""sentiment analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18YwJgNuknvAeTwesLKF22itKUV0xKU4a

## Sentiment Analysis for Sexist Language in LinkedIn Job Ads
### Berra Karayel
### 0054477
"""

pip install pandas matplotlib tensorflow

import pandas as pd
import matplotlib.pyplot as plt

linkedinsentimentdata = pd.read_csv("/content/linkedin sentiment data annotatedd 2.csv")
linkedinsentimentdata.head(10)

sentiment_linkedin = linkedinsentimentdata[["Requirements_processed","Sentiment"]]

print(sentiment_linkedin.shape)
sentiment_linkedin.head(5)

sentiment_linkedin.columns

sentiment_linkedin = sentiment_linkedin[sentiment_linkedin["Sentiment"] != 'neutral']

print(sentiment_linkedin.shape)
sentiment_linkedin.head(5)

sentiment_linkedin["Sentiment"].value_counts()

sentiment_label = sentiment_linkedin.Sentiment.factorize()    #converting categorical values to numerical
sentiment_label                                               # 0 represents positive 1 represents negative here

sentiment_linkedin

corpus = sentiment_linkedin.Requirements_processed.values        #converting the text into an array of vector embeddings

corpus

from tensorflow.keras.preprocessing.text import Tokenizer

from tensorflow.keras.preprocessing.sequence import pad_sequences

tokenizer = Tokenizer(num_words=5000)

tokenizer.fit_on_texts(str(corpus))
vocab_size = len(tokenizer.word_index) + 1
encoded_docs = tokenizer.texts_to_sequences(str(corpus))
padded_sequence = pad_sequences(encoded_docs, maxlen=200)

"""## Building Text Classifier

For the machine learning model, LSTM layers were used. The architecture of my model consists of an embedding layer, an LSTM layer, and a Dense layer at the end. To avoid overfitting, I introduced the Dropout mechanism in-between the LSTM layers as a regularization technique. With this way, I created a robust model avoiding overfitting.
"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM,Dense, Dropout, SpatialDropout1D
from tensorflow.keras.layers import Embedding


embedding_vector_length = 32
model = Sequential()
model.add(Embedding(vocab_size, embedding_vector_length, input_length=200))
model.add(SpatialDropout1D(0.25))
model.add(LSTM(50, dropout=0.5, recurrent_dropout=0.5))
model.add(Dropout(0.2))
model.add(Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])

print(model.summary())

"""## Training Sentiment Analysis Model"""

import numpy as np

history = model.fit(padded_sequence, sentiment_label[0],validation_split= 0.2, epochs=10, batch_size=32)

import matplotlib.pyplot as plt

plt.plot(history.history['accuracy'], label='acc')
plt.plot(history.history['val_accuracy'], label='val_acc')
plt.legend()
plt.show()

plt.savefig("Accuracy plot.jpg")

plt.plot(history.history['loss'], label='loss')
plt.plot(history.history['val_loss'], label='val_loss')

plt.legend()
plt.show()

plt.savefig("Loss plt.jpg")

def predict_sentiment(text):
    tw = tokenizer.texts_to_sequences([text])
    tw = pad_sequences(tw,maxlen=200)
    prediction = int(model.predict(tw).round().item())
    print("Predicted label: ", sentiment_label[1][prediction])


test_sentence1 = input("Insert the job ad here to check whether it is masculine or feminine: ")
predict_sentiment(test_sentence1)